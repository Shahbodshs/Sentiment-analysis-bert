{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4140,"sourceType":"datasetVersion","datasetId":2477}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"DATASET_COLUMNS  = [\"sentiment\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\nDATASET_ENCODING = \"ISO-8859-1\"","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:01:48.655288Z","iopub.execute_input":"2024-08-20T12:01:48.655824Z","iopub.status.idle":"2024-08-20T12:01:48.659964Z","shell.execute_reply.started":"2024-08-20T12:01:48.655796Z","shell.execute_reply":"2024-08-20T12:01:48.659116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport pickle\nimport numpy as np\nimport pandas as pd\n# Plot libraries\nimport seaborn as sns\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n# Torch ML libraries\nimport tensorflow as tf\nfrom transformers import AutoTokenizer\n\n# Keras \nfrom sklearn.model_selection import train_test_split\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-20T12:01:30.565385Z","iopub.execute_input":"2024-08-20T12:01:30.565769Z","iopub.status.idle":"2024-08-20T12:01:48.653770Z","shell.execute_reply.started":"2024-08-20T12:01:30.565738Z","shell.execute_reply":"2024-08-20T12:01:48.653011Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset =  pd.read_csv('../input/sentiment140/training.1600000.processed.noemoticon.csv'\n                      ,encoding = DATASET_ENCODING , names= DATASET_COLUMNS)","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:01:48.661238Z","iopub.execute_input":"2024-08-20T12:01:48.661568Z","iopub.status.idle":"2024-08-20T12:01:55.670013Z","shell.execute_reply.started":"2024-08-20T12:01:48.661536Z","shell.execute_reply":"2024-08-20T12:01:55.669242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = dataset[['sentiment' , 'text']]\ndataset = dataset.sample(frac=1, random_state=42).reset_index(drop=True)\ndataset = dataset.head(100000)\nlabel_counts = dataset['sentiment'].value_counts()\nprint(label_counts)","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:01:55.671944Z","iopub.execute_input":"2024-08-20T12:01:55.672277Z","iopub.status.idle":"2024-08-20T12:01:56.123984Z","shell.execute_reply.started":"2024-08-20T12:01:55.672244Z","shell.execute_reply":"2024-08-20T12:01:56.122972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def change_label(value): \n    if value == 4: \n        return 1\n    else: \n        return value","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:01:56.124959Z","iopub.execute_input":"2024-08-20T12:01:56.125251Z","iopub.status.idle":"2024-08-20T12:01:56.129891Z","shell.execute_reply.started":"2024-08-20T12:01:56.125213Z","shell.execute_reply":"2024-08-20T12:01:56.128921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining regex patterns.\nurlPattern        = r\"((http://)[^ ]*|(https://)[^ ]*|(www\\.)[^ ]*)\"\nuserPattern       = '@[^\\s]+'\nhashtagPattern    = '#[^\\s]+'\nsequencePattern   = r\"(.)\\1\\1+\"\nseqReplacePattern = r\"\\1\\1\"\n\n\ndef preprocess_apply(tweet):\n\n    tweet = tweet.lower()\n\n    # Replace all URls with '<url>'\n    tweet = re.sub(urlPattern,'',tweet)\n    # Replace @USERNAME to '<user>'.\n    tweet = re.sub(userPattern,'', tweet)\n    \n    # Replace 3 or more consecutive letters by 2 letter.\n    tweet = re.sub(sequencePattern, seqReplacePattern, tweet)\n\n    # Adding space on either side of '/' to seperate words (After replacing URLS).\n    tweet = re.sub(r'/', ' / ', tweet)\n    return tweet","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:01:56.130942Z","iopub.execute_input":"2024-08-20T12:01:56.131253Z","iopub.status.idle":"2024-08-20T12:01:56.139051Z","shell.execute_reply.started":"2024-08-20T12:01:56.131220Z","shell.execute_reply":"2024-08-20T12:01:56.138010Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset['processed_text'] = dataset.text.apply(preprocess_apply)\ndataset['sentiment'] = dataset['sentiment'].apply(change_label)","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:01:56.140262Z","iopub.execute_input":"2024-08-20T12:01:56.140510Z","iopub.status.idle":"2024-08-20T12:01:58.795678Z","shell.execute_reply.started":"2024-08-20T12:01:56.140488Z","shell.execute_reply":"2024-08-20T12:01:58.794875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset['word_count'] = dataset['processed_text'].apply(lambda x: len(str(x).split()))\n\n# Calculate the statistics\nplt.figure(figsize=(8, 6))\nsns.violinplot(x=dataset['word_count'])\nplt.title('Violin Plot of Word Count per Tweet')\nplt.xlabel('Word Count')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:01:58.796842Z","iopub.execute_input":"2024-08-20T12:01:58.797192Z","iopub.status.idle":"2024-08-20T12:01:59.402051Z","shell.execute_reply.started":"2024-08-20T12:01:58.797159Z","shell.execute_reply":"2024-08-20T12:01:59.401093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset['sentiment'].nunique()","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:01:59.403615Z","iopub.execute_input":"2024-08-20T12:01:59.404255Z","iopub.status.idle":"2024-08-20T12:01:59.413481Z","shell.execute_reply.started":"2024-08-20T12:01:59.404220Z","shell.execute_reply":"2024-08-20T12:01:59.412280Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('distilbert-base-cased')","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:01:59.418074Z","iopub.execute_input":"2024-08-20T12:01:59.418537Z","iopub.status.idle":"2024-08-20T12:02:00.179478Z","shell.execute_reply.started":"2024-08-20T12:01:59.418509Z","shell.execute_reply":"2024-08-20T12:02:00.178587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"texts = list(dataset['processed_text'])\nlabels = list(dataset['sentiment'])","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:02:00.180569Z","iopub.execute_input":"2024-08-20T12:02:00.180851Z","iopub.status.idle":"2024-08-20T12:02:00.209762Z","shell.execute_reply.started":"2024-08-20T12:02:00.180826Z","shell.execute_reply":"2024-08-20T12:02:00.209001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoded_inputs = tokenizer(\n    texts,\n    max_length=40,            # Set maximum sequence length to 40 tokens\n    padding='max_length',     # Pad sequences to the maximum length\n    truncation=True,          # Truncate sequences longer than the maximum length\n    return_tensors='np'       # Return NumPy arrays (use 'tf' for TensorFlow tensors directly)\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:02:00.210691Z","iopub.execute_input":"2024-08-20T12:02:00.210982Z","iopub.status.idle":"2024-08-20T12:02:05.839935Z","shell.execute_reply.started":"2024-08-20T12:02:00.210958Z","shell.execute_reply":"2024-08-20T12:02:05.839132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_ids = encoded_inputs['input_ids']\nattention_mask = encoded_inputs['attention_mask']","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:02:05.841489Z","iopub.execute_input":"2024-08-20T12:02:05.841896Z","iopub.status.idle":"2024-08-20T12:02:05.846218Z","shell.execute_reply.started":"2024-08-20T12:02:05.841841Z","shell.execute_reply":"2024-08-20T12:02:05.845375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Input IDs:\", input_ids[0])\nprint(\"Attention Mask:\", attention_mask[0])","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:02:05.847385Z","iopub.execute_input":"2024-08-20T12:02:05.847658Z","iopub.status.idle":"2024-08-20T12:02:05.856637Z","shell.execute_reply.started":"2024-08-20T12:02:05.847636Z","shell.execute_reply":"2024-08-20T12:02:05.855775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame({\n    'input_ids': list(input_ids),\n    'attention_mask': list(attention_mask),\n    'labels': labels  # Ensure this is a list or array\n})","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:02:05.857653Z","iopub.execute_input":"2024-08-20T12:02:05.857925Z","iopub.status.idle":"2024-08-20T12:02:05.969317Z","shell.execute_reply.started":"2024-08-20T12:02:05.857903Z","shell.execute_reply":"2024-08-20T12:02:05.968439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train, df_temp = train_test_split(df, test_size=0.2, random_state=42, shuffle = True)\n# Further split temp into validation and test sets\ndf_val, df_test = train_test_split(df_temp, test_size=0.5, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:02:05.971097Z","iopub.execute_input":"2024-08-20T12:02:05.971522Z","iopub.status.idle":"2024-08-20T12:02:05.989997Z","shell.execute_reply.started":"2024-08-20T12:02:05.971485Z","shell.execute_reply":"2024-08-20T12:02:05.989309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_tf_dataset_from_df(df, batch_size=32):\n    input_ids = tf.convert_to_tensor(np.array(df['input_ids'].tolist()), dtype=tf.int32)\n    attention_mask = tf.convert_to_tensor(np.array(df['attention_mask'].tolist()), dtype=tf.int32)\n    labels = tf.convert_to_tensor(np.array(df['labels']), dtype=tf.int32)\n\n    dataset = tf.data.Dataset.from_tensor_slices((\n        {\n            'input_ids': input_ids,\n            'attention_mask': attention_mask\n        },\n        labels\n    ))\n    dataset = dataset.batch(batch_size)\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:02:05.991196Z","iopub.execute_input":"2024-08-20T12:02:05.991469Z","iopub.status.idle":"2024-08-20T12:02:05.997426Z","shell.execute_reply.started":"2024-08-20T12:02:05.991446Z","shell.execute_reply":"2024-08-20T12:02:05.996579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset_tf = create_tf_dataset_from_df(df_train, batch_size=32)\nval_dataset_tf = create_tf_dataset_from_df(df_val, batch_size=32)\ntest_dataset_tf = create_tf_dataset_from_df(df_test , batch_size = 32)","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:02:05.998713Z","iopub.execute_input":"2024-08-20T12:02:05.999339Z","iopub.status.idle":"2024-08-20T12:02:06.838027Z","shell.execute_reply.started":"2024-08-20T12:02:05.999311Z","shell.execute_reply":"2024-08-20T12:02:06.837251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import TFDistilBertForSequenceClassification\n\nmodel = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-cased', num_labels=2)","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:02:06.839063Z","iopub.execute_input":"2024-08-20T12:02:06.839355Z","iopub.status.idle":"2024-08-20T12:02:13.467751Z","shell.execute_reply.started":"2024-08-20T12:02:06.839330Z","shell.execute_reply":"2024-08-20T12:02:13.467021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nmodel.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n\n# Step 5: Model Training\nhistory = model.fit(\n    train_dataset_tf,\n    epochs=2,\n    validation_data=val_dataset_tf\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:02:13.468910Z","iopub.execute_input":"2024-08-20T12:02:13.469260Z","iopub.status.idle":"2024-08-20T12:14:54.090893Z","shell.execute_reply.started":"2024-08-20T12:02:13.469233Z","shell.execute_reply":"2024-08-20T12:14:54.090075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_loss, test_accuracy = model.evaluate(test_dataset_tf)\n\nprint(f\"Test Loss: {test_loss}\")\nprint(f\"Test Accuracy: {test_accuracy}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:17:27.333440Z","iopub.execute_input":"2024-08-20T12:17:27.333842Z","iopub.status.idle":"2024-08-20T12:17:41.833469Z","shell.execute_reply.started":"2024-08-20T12:17:27.333811Z","shell.execute_reply":"2024-08-20T12:17:41.832525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Step 1: Extract actual labels from the test dataset\nactual_labels = []\nfor _, labels in test_dataset_tf:\n    actual_labels.extend(labels.numpy())\n\n# Convert actual labels to a NumPy array\nactual_labels = np.array(actual_labels)\n\n# Step 2: Generate predictions from the model\npredictions = model.predict(test_dataset_tf)\npredicted_labels = np.argmax(predictions.logits, axis=-1)\n\n# Step 3: Calculate the confusion matrix\nconf_matrix = confusion_matrix(actual_labels, predicted_labels)\n\n# Step 4: Plot the confusion matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.show()\n\n# Step 5: Print the classification report\nclass_report = classification_report(actual_labels, predicted_labels, target_names=['Class 0', 'Class 1'])\nprint(\"Classification Report:\")\nprint(class_report)","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:23:10.625465Z","iopub.execute_input":"2024-08-20T12:23:10.625866Z","iopub.status.idle":"2024-08-20T12:23:25.479131Z","shell.execute_reply.started":"2024-08-20T12:23:10.625829Z","shell.execute_reply":"2024-08-20T12:23:25.478245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save_pretrained(\"./sentiment_bert\")","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:30:46.875126Z","iopub.execute_input":"2024-08-20T12:30:46.875989Z","iopub.status.idle":"2024-08-20T12:30:47.632773Z","shell.execute_reply.started":"2024-08-20T12:30:46.875955Z","shell.execute_reply":"2024-08-20T12:30:47.631884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"load = TFDistilBertForSequenceClassification.from_pretrained('/kaggle/working/sentiment_bert')","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:37:57.081414Z","iopub.execute_input":"2024-08-20T12:37:57.081793Z","iopub.status.idle":"2024-08-20T12:37:57.775067Z","shell.execute_reply.started":"2024-08-20T12:37:57.081766Z","shell.execute_reply":"2024-08-20T12:37:57.774187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentence = 'Great, another monday...'\nencoded_sent = tokenizer(\n    sentence,\n    max_length=40,            # Set maximum sequence length to 40 tokens\n    padding='max_length',     # Pad sequences to the maximum length\n    truncation=True,          # Truncate sequences longer than the maximum length\n    return_tensors='np'       # Return NumPy arrays (use 'tf' for TensorFlow tensors directly)\n)\ntf_output = load.predict(encoded_sent)[0]\ntf_prediction = tf.nn.softmax(tf_output, axis=1).numpy()[0]\ntf_prediction","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:42:08.457889Z","iopub.execute_input":"2024-08-20T12:42:08.458230Z","iopub.status.idle":"2024-08-20T12:42:08.548595Z","shell.execute_reply.started":"2024-08-20T12:42:08.458190Z","shell.execute_reply":"2024-08-20T12:42:08.547726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see, the model still was able to predict the sentence with sarcasm to a correct class which was 0 which indicates the negative sentiment.  ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}